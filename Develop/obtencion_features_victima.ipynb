{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renaming_keys(lines):\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = lines[f\"page\"+str(i)]\n",
    "        del lines[f\"page\"+str(i)]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text_sentence(dictionary_file):\n",
    "    nested_text = \"\"\n",
    "    for key, value in sorted(dictionary_file.items(), key=lambda item: int(item[0])):\n",
    "        nested_text = nested_text+\" \"+dictionary_file[key] \n",
    "    return nested_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text(nested_text):\n",
    "     nested_text = nested_text.replace('\\n', '')\n",
    "     nested_text = nested_text.lower()\n",
    "     import unicodedata\n",
    "     nested_text_1 = nested_text.replace(\"ñ\", \"#\").replace(\"Ñ\", \"%\")\n",
    "     clean_text = unicodedata.normalize(\"NFKD\", nested_text_1)\\\n",
    "          .encode(\"ascii\",\"ignore\").decode(\"ascii\")\\\n",
    "          .replace(\"#\", \"ñ\").replace(\"%\", \"Ñ\")\n",
    "     return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_text_subextract(clean_text, list_keywords_victim,before_vecindad,after_vecindad):\n",
    "    list_text_subextract = [] \n",
    "    for list_victims_word in list_keywords_victim:\n",
    "        for m in re.finditer(list_victims_word, clean_text):\n",
    "            start=int(m.start()-before_vecindad)\n",
    "            end=int(m.end()+after_vecindad)\n",
    "            list_text_subextract.append(clean_text[start:end])\n",
    "    return list_text_subextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_subtext(list_keywords,before_vecindad,after_vecindad):\n",
    "#     path = 'output_txt_v1/output_txt/'\n",
    "#     files = os.listdir(path)\n",
    "#     total_text = ''\n",
    "#     list_text_subextract = []\n",
    "#     for file in files:\n",
    "#         print(f'Reading file {file}')\n",
    "#         with open(path+file) as f:\n",
    "#             lines = f.readlines()\n",
    "#             lines=lines[0]\n",
    "#             lines = json.loads(lines)\n",
    "#             dictionary_file = renaming_keys(lines)\n",
    "#             nested_text = concat_text_sentence(dictionary_file)\n",
    "#             clean_text = get_clean_text(nested_text)\n",
    "#             list_document_list_victims = get_list_text_subextract(clean_text,list_keywords,before_vecindad,after_vecindad)\n",
    "#             list_text_subextract.extend(list_document_list_victims)\n",
    "#     return list_text_subextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_subtext(list_keywords,before_vecindad,after_vecindad):\n",
    "    path = 'output_txt_v1/output_txt/'\n",
    "    file = 'textos.txt'\n",
    "    total_text = ''\n",
    "    list_text_subextract = []\n",
    "    print(f'Reading file {file}')\n",
    "    with open(path+file) as f:\n",
    "        lines = f.readlines()\n",
    "        nested_text = \" \"\n",
    "        for line in lines :\n",
    "            nested_text = nested_text +\" \"+line\n",
    "        clean_text = get_clean_text(nested_text)\n",
    "        list_document_list_victims = get_list_text_subextract(clean_text,list_keywords,before_vecindad,after_vecindad)\n",
    "        list_text_subextract.extend(list_document_list_victims)\n",
    "        return list_text_subextract\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file textos.txt\n",
      "Reading file textos.txt\n",
      "Reading file textos.txt\n",
      "Reading file textos.txt\n",
      "Reading file textos.txt\n"
     ]
    }
   ],
   "source": [
    "#### Victimario\n",
    "list_keywords = [\"eln\", \"grupo armado\", \"farc\", \"elp\", \"bacrim\", \"paramilitares\", \"auc\", \"guerrilla\", \"ejército\", \"autodefensas\", \"fuerza pública\", \" acuu\", \"fuerzas armadas\"]\n",
    "before_vecindad = 150\n",
    "after_vecindad = 300\n",
    "list_victimario = extract_subtext(list_keywords,before_vecindad,after_vecindad) \n",
    "\n",
    "\n",
    "#### Tipologia perdida del bien\n",
    "list_keywords = [\"abandono\", \"despojo\", \"desplazamiento\", \"forzado\", \"tipología\", \"perdida\"]\n",
    "before_vecindad = 150\n",
    "after_vecindad = 300\n",
    "list_perdida = extract_subtext(list_keywords,before_vecindad,after_vecindad)  \n",
    "\n",
    "#### Solicitante\n",
    "list_keywords = [\"solicitante\", \"reclamante\", \"accionante\", \"demandante\"]\n",
    "before_vecindad = 150\n",
    "after_vecindad = 300\n",
    "list_solicitante = extract_subtext(list_keywords,before_vecindad,after_vecindad)  \n",
    "\n",
    "\n",
    "#### Sentido de la decisión\n",
    "list_keywords = [\"restitución\", \"restituye\", \"formaliza\", \"formalización\", \"negar\", \"proteger\", \"ordenar\", \"restituir\", \"decretar\", \"amparar\", \"petición\", \"nieguese\", \"nieganse\"]\n",
    "before_vecindad = 150\n",
    "after_vecindad = 300\n",
    "list_decision = extract_subtext(list_keywords,before_vecindad,after_vecindad)  \n",
    "\n",
    "#### Ubicación\n",
    "list_keywords = [\"departamento\", \"municipio\", \"vereda\", \"urbano\", \"rural\", \"matricula\", \"inmobiliaria\"]\n",
    "before_vecindad = 150\n",
    "after_vecindad = 300\n",
    "list_ubicacion = extract_subtext(list_keywords,before_vecindad,after_vecindad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1786\n",
      "2785\n",
      "2195\n",
      "1492\n",
      "4594\n"
     ]
    }
   ],
   "source": [
    "print(len(list_victimario))\n",
    "print(len(list_perdida))\n",
    "print(len(list_solicitante))\n",
    "print(len(list_decision))\n",
    "print(len(list_ubicacion))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'sentences': list_victimario}\n",
    "victima_text = pd.DataFrame(data=d)\n",
    "victima_text.to_csv(path_or_buf=\"list_victimario.csv\",index=False)\n",
    "\n",
    "d = {'sentences': list_perdida}\n",
    "victimario_text = pd.DataFrame(data=d)\n",
    "victimario_text.to_csv(path_or_buf=\"list_perdida.csv\",index=False)\n",
    "\n",
    "d = {'sentences': list_solicitante}\n",
    "victimario_text = pd.DataFrame(data=d)\n",
    "victimario_text.to_csv(path_or_buf=\"list_solicitante.csv\",index=False)\n",
    "\n",
    "d = {'sentences': list_decision}\n",
    "victimario_text = pd.DataFrame(data=d)\n",
    "victimario_text.to_csv(path_or_buf=\"list_decision.csv\",index=False)\n",
    "\n",
    "d = {'sentences': list_ubicacion}\n",
    "victimario_text = pd.DataFrame(data=d)\n",
    "victimario_text.to_csv(path_or_buf=\"list_ubicacion.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
